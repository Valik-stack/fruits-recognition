{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0db6c3f-b9cb-4071-a377-40dd4b9e1134",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 926 images belonging to 30 classes.\n",
      "Found 373 images belonging to 30 classes.\n",
      "Epoch 1/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.1053 - loss: 7.6629 - val_accuracy: 0.3940 - val_loss: 2.2886\n",
      "Epoch 2/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3750 - loss: 2.4646 - val_accuracy: 0.6000 - val_loss: 2.1727\n",
      "Epoch 3/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.2614 - loss: 2.5885 - val_accuracy: 0.5924 - val_loss: 1.6084\n",
      "Epoch 4/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3125 - loss: 2.2398 - val_accuracy: 0.8000 - val_loss: 1.8595\n",
      "Epoch 5/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.4056 - loss: 2.0533 - val_accuracy: 0.7065 - val_loss: 1.4170\n",
      "Epoch 6/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3750 - loss: 2.1677 - val_accuracy: 0.6000 - val_loss: 1.3754\n",
      "Epoch 7/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.4870 - loss: 1.7609 - val_accuracy: 0.7418 - val_loss: 1.1276\n",
      "Epoch 8/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5000 - loss: 1.8520 - val_accuracy: 0.8000 - val_loss: 1.5579\n",
      "Epoch 9/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.5325 - loss: 1.6930 - val_accuracy: 0.7989 - val_loss: 0.8726\n",
      "Epoch 10/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5000 - loss: 1.6926 - val_accuracy: 0.8000 - val_loss: 1.1298\n",
      "Epoch 11/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.5738 - loss: 1.3933 - val_accuracy: 0.8152 - val_loss: 0.7977\n",
      "Epoch 12/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8125 - loss: 0.7665 - val_accuracy: 1.0000 - val_loss: 0.3960\n",
      "Epoch 13/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.6372 - loss: 1.2619 - val_accuracy: 0.8370 - val_loss: 0.6903\n",
      "Epoch 14/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2500 - loss: 2.5310 - val_accuracy: 0.8000 - val_loss: 0.5675\n",
      "Epoch 15/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 1s/step - accuracy: 0.6056 - loss: 1.2662 - val_accuracy: 0.8533 - val_loss: 0.5411\n",
      "Epoch 16/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5000 - loss: 1.1282 - val_accuracy: 1.0000 - val_loss: 0.3726\n",
      "Epoch 17/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.7080 - loss: 0.9900 - val_accuracy: 0.8777 - val_loss: 0.4667\n",
      "Epoch 18/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8125 - loss: 0.6095 - val_accuracy: 1.0000 - val_loss: 0.3244\n",
      "Epoch 19/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 1s/step - accuracy: 0.6710 - loss: 1.0249 - val_accuracy: 0.9239 - val_loss: 0.3958\n",
      "Epoch 20/20\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5625 - loss: 1.2217 - val_accuracy: 1.0000 - val_loss: 0.6600\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 782ms/step - accuracy: 0.9360 - loss: 0.3888\n",
      "Validation Loss: 0.38659653067588806\n",
      "Validation Accuracy: 0.9276139140129089\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Path to data with images\n",
    "data_dir = 'Data'\n",
    "train_dir = 'TrainData'\n",
    "val_dir = 'ValData'\n",
    "\n",
    "# Creation of directories for train and validation data if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Function to automatically split images into train and validation directories\n",
    "def split_data_into_train_val(data_dir, train_dir, val_dir, split_ratio=0.8):\n",
    "    classes = os.listdir(data_dir)\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(data_dir, cls)\n",
    "        if os.path.isdir(cls_dir):\n",
    "            images = os.listdir(cls_dir)\n",
    "            random.shuffle(images)\n",
    "            split_index = int(len(images) * split_ratio)\n",
    "            train_images = images[:split_index]\n",
    "            val_images = images[split_index:]\n",
    "            for img in train_images:\n",
    "                src = os.path.join(cls_dir, img)\n",
    "                dst = os.path.join(train_dir, cls)\n",
    "                os.makedirs(dst, exist_ok=True)\n",
    "                shutil.copy(src, dst)\n",
    "            for img in val_images:\n",
    "                src = os.path.join(cls_dir, img)\n",
    "                dst = os.path.join(val_dir, cls)\n",
    "                os.makedirs(dst, exist_ok=True)\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "# Splitting data into train and validation directories\n",
    "split_data_into_train_val(data_dir, train_dir, val_dir)\n",
    "\n",
    "# Defining image dimensions and batch size\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 16\n",
    "\n",
    "# Creati data generators with augmentation for training and validation\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Loading bas model VGG16 \n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# Freezing convolutional layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Creating new classification layers\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(1024, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(train_generator.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "# Model compiling\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "loss, accuracy = model.evaluate(val_generator)\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation Accuracy:\", accuracy)\n",
    "\n",
    "model.save('fruit_classification_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41748e9e-ae24-4ba1-958b-313bc64ad335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
